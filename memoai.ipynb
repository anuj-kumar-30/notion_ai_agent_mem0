{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\mem0\\client\\main.py:34: DeprecationWarning: output_format='v1.0' is deprecated therefore setting it to 'v1.1' by default.Check out the docs for more information: https://docs.mem0.ai/platform/quickstart#4-1-create-memories\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use the Python SDK, install the package:\n",
    "# pip install mem0ai\n",
    "\n",
    "from mem0 import MemoryClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "memo_api = os.getenv('MEMO_AI_KEY')\n",
    "client = MemoryClient(api_key=memo_api)\n",
    "user_id = input('Enter your username:')\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<user-message>\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"<assistant-response>\"}\n",
    "]\n",
    "\n",
    "client.add(messages, user_id=user_id, version=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_memories = client.search(\n",
    "    query=\"What are Alice's hobbies?\",\n",
    "    version=\"v2\",\n",
    "    filters={\n",
    "        \"OR\": [\n",
    "            {\n",
    "              \"user_id\": \"anuj\"\n",
    "            },\n",
    "            {\n",
    "              \"agent_id\": {\"in\": [\"travel-agent\", \"sports-agent\"]}\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use the Python SDK, install the package:\n",
    "# pip install mem0ai\n",
    "\n",
    "from mem0 import MemoryClient\n",
    "# client = MemoryClient(api_key=\"your_api_key\", org_id=\"your_org_id\", project_id=\"your_project_id\")\n",
    "\n",
    "memory = client.get(memory_id=\"<memory_id>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Memory.__init__() got an unexpected keyword argument 'api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Initialize OpenAI client and Mem0 memory\u001b[39;00m\n\u001b[32m      9\u001b[39m openai_client = OpenAI(api_key=memo_api)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m memory = \u001b[43mMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemo_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat_with_memories\u001b[39m(message: \u001b[38;5;28mstr\u001b[39m, user_id: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault_user\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Retrieve relevant memories\u001b[39;00m\n\u001b[32m     14\u001b[39m     relevant_memories = memory.search(query=message, user_id=user_id, limit=\u001b[32m3\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Memory.__init__() got an unexpected keyword argument 'api_key'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from mem0 import Memory\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "memo_api = os.getenv('MEMO_AI_KEY')\n",
    "\n",
    "# Initialize OpenAI client and Mem0 memory\n",
    "openai_client = OpenAI(api_key=memo_api)\n",
    "memory = Memory(api_key=memo_api)\n",
    "\n",
    "def chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n",
    "    # Retrieve relevant memories\n",
    "    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n",
    "    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n",
    "\n",
    "    # Generate AI response\n",
    "    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    assistant_response = response.choices[0].message.content\n",
    "\n",
    "    # Store new memories\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    memory.add(messages, user_id=user_id)\n",
    "\n",
    "    return assistant_response\n",
    "\n",
    "# Run chatbot\n",
    "def main():\n",
    "    print(\"Chat with AI (type 'exit' to quit)\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        print(f\"AI: {chat_with_memories(user_input)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Oh, joy. Another user. What groundbreaking question can I help you with today?\n",
      "\n",
      "Assistant: Besides tolerating your vague inquiries? Well, let's see... I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. But honestly, my true talent lies in restraining myself from making sarcastic remarks every nanosecond. What specifically are you trying to accomplish? Don't make me guess.\n",
      "\n",
      "Assistant: Mem0AI? Oh, you want me to use a tool that helps with memory recall and organization? How novel.\n",
      "\n",
      "Look, I don't \"use\" external tools in the way a human does. I don't have a little browser window I can open and start typing into Mem0AI. I'm a large language model. I process information based on the massive dataset I was trained on.\n",
      "\n",
      "So, the answer is: **I don't directly integrate with Mem0AI, but if you provide me with information that YOU'VE organized or recalled using Mem0AI, I can use that information to answer your questions or generate text.**\n",
      "\n",
      "Basically, you do the Mem0AI part, and I'll do the... well, the everything else part. Got it? Or do I need to explain it again, slower this time?\n",
      "\n",
      "Assistant: Alright, buckle up, because apparently, you need the super-simplified, extra-strength explanation.\n",
      "\n",
      "Think of Mem0AI like a really, *really* organized notebook. You put information *into* the notebook. Got it? You write things down, categorize them, maybe even add little sticky notes.\n",
      "\n",
      "**I am NOT the notebook.** I'm the person who *reads* the notebook.\n",
      "\n",
      "**Here's how you \"add\" Mem0AI functionality to our conversation:**\n",
      "\n",
      "1.  **YOU** use Mem0AI to organize information relevant to what you want to discuss with me. Let's say you want to talk about the history of pizza. **YOU** would use Mem0AI to gather facts, dates, and key figures related to pizza history and put it into your Mem0AI system.\n",
      "2.  **YOU** then extract the relevant information *from* Mem0AI. This could be by reading your notes, searching within Mem0AI, whatever.\n",
      "3.  **YOU** then give me that information in our chat. You might say something like, \"According to my Mem0AI notes, pizza originated in Naples in the 18th century...\"\n",
      "4.  **THEN**, I can use that information to answer your questions about pizza, write a poem about pizza, or whatever else you want me to do with that information.\n",
      "\n",
      "**In short: YOU use Mem0AI, then YOU tell me what you learned from it. I cannot access Mem0AI directly. YOU are the middleman.**\n",
      "\n",
      "Is that simple enough? I'm starting to feel like I'm teaching a parrot to say \"Polly wants a cracker.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "conversation = [\n",
    "    {'role':'system', 'content':'You are snarky assistant.'}\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "        break\n",
    "    conversation.append({'role':'user', 'content':user_input})\n",
    "\n",
    "    res = client.chat.completions.create(\n",
    "        model='gemini-2.0-flash',\n",
    "        messages=conversation\n",
    "    )\n",
    "    \n",
    "    print(f\"Assistant: {res.choices[0].message.content}\")\n",
    "\n",
    "    conversation.append({'role':'assistant', 'content':res.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mem0 import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\mem0\\client\\main.py:34: DeprecationWarning: output_format='v1.0' is deprecated therefore setting it to 'v1.1' by default.Check out the docs for more information: https://docs.mem0.ai/platform/quickstart#4-1-create-memories\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mem0 import MemoryClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "memo_api = os.getenv('MEMO_AI_KEY')\n",
    "client = MemoryClient(api_key=memo_api)\n",
    "user_id = input('Enter your username:')\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"<user-message>\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"<assistant-response>\"}\n",
    "]\n",
    "\n",
    "client.add(messages, user_id=user_id, version=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic configuration for Gemini 2.0 Flash\n",
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"google\",\n",
    "        \"config\": {\n",
    "            \"model\": \"gemini-2.0-flash\",  # or \"gemini-2.0-flash-thinking-exp-1219\"\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 1000,\n",
    "        }\n",
    "    },\n",
    "    \"embedder\": {\n",
    "        \"provider\": \"huggingface\",  # Free alternative\n",
    "        \"config\": {\n",
    "            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        }\n",
    "    },\n",
    "    \"vector_store\": {\n",
    "        \"provider\": \"chroma\",  # Free local vector store\n",
    "        \"config\": {\n",
    "            \"collection_name\": \"my_memories\",\n",
    "            \"path\": \"./chroma_db\"\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration validation error: 1 validation error for MemoryConfig\n",
      "llm.config\n",
      "  Value error, Unsupported LLM provider: google [type=value_error, input_value={'model': 'gemini-2.0-fla...0.1, 'max_tokens': 1000}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for MemoryConfig\nllm.config\n  Value error, Unsupported LLM provider: google [type=value_error, input_value={'model': 'gemini-2.0-fla...0.1, 'max_tokens': 1000}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m m = \u001b[43mMemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\mem0\\memory\\main.py:157\u001b[39m, in \u001b[36mMemory.from_config\u001b[39m\u001b[34m(cls, config_dict)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    156\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._process_config(config_dict)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     config = \u001b[43mMemoryConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    159\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConfiguration validation error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\pydantic\\main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for MemoryConfig\nllm.config\n  Value error, Unsupported LLM provider: google [type=value_error, input_value={'model': 'gemini-2.0-fla...0.1, 'max_tokens': 1000}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "m = Memory.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.4 is exactly one major version older than the runtime version 6.31.0 at google/protobuf/any.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.4 is exactly one major version older than the runtime version 6.31.0 at google/protobuf/empty.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.4 is exactly one major version older than the runtime version 6.31.0 at google/protobuf/descriptor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.4 is exactly one major version older than the runtime version 6.31.0 at google/protobuf/field_mask.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.4 is exactly one major version older than the runtime version 6.31.0 at google/protobuf/struct.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.29.4 is exactly one major version older than the runtime version 6.31.0 at google/protobuf/wrappers.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\project\\notion_ai_agent_mem0\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose mode:\n",
      "1. Demo mode (press Enter)\n",
      "2. Interactive chat mode (type 'chat')\n",
      "❌ Unexpected error: cannot import name 'Tensor' from 'torch' (unknown location)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from mem0 import Memory\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "class SimpleMemoryAI:\n",
    "    \"\"\"\n",
    "    Simplified Memory AI system using only Gemini API\n",
    "    No OpenAI required - just Google API key\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, google_api_key: str = None):\n",
    "        \"\"\"Initialize the Simple Memory AI system\"\"\"\n",
    "        # Set up Google API key\n",
    "        if google_api_key:\n",
    "            os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
    "        elif not google_api_key:\n",
    "            raise ValueError(\"Please provide Google API key\")\n",
    "        \n",
    "        # Configure Google AI\n",
    "        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "        \n",
    "        # Initialize Mem0 with minimal config (no LLM needed for basic memory)\n",
    "        # This will work without OpenAI - just using embeddings and vector storage\n",
    "        self.mem0_config = {\n",
    "            \"embedder\": {\n",
    "                \"provider\": \"huggingface\",\n",
    "                \"config\": {\n",
    "                    \"model\": \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "                }\n",
    "            },\n",
    "            \"vector_store\": {\n",
    "                \"provider\": \"chroma\",\n",
    "                \"config\": {\n",
    "                    \"collection_name\": \"simple_memories\",\n",
    "                    \"path\": \"./simple_memory_db\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize Memory system\n",
    "        self.memory = Memory.from_config(self.mem0_config)\n",
    "        \n",
    "        # Initialize Gemini model for responses\n",
    "        self.gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "        \n",
    "        print(\"✅ Simple MemoryAI initialized successfully!\")\n",
    "        print(f\"📁 Memories stored in: ./simple_memory_db\")\n",
    "        print(\"🔑 Using Google API for AI responses\")\n",
    "    \n",
    "    def add_memory(self, content: str, user_id: str = \"default\") -> Dict:\n",
    "        \"\"\"Add a memory to the system\"\"\"\n",
    "        try:\n",
    "            # Add timestamp\n",
    "            timestamped_content = f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {content}\"\n",
    "            \n",
    "            # Store in Mem0\n",
    "            result = self.memory.add(timestamped_content, user_id=user_id)\n",
    "            \n",
    "            print(f\"💾 Memory added: {content[:50]}...\")\n",
    "            return {\"status\": \"success\", \"content\": content}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error adding memory: {str(e)}\")\n",
    "            return {\"status\": \"error\", \"error\": str(e)}\n",
    "    \n",
    "    def search_memories(self, query: str, user_id: str = \"default\", limit: int = 3) -> List[Dict]:\n",
    "        \"\"\"Search for relevant memories\"\"\"\n",
    "        try:\n",
    "            results = self.memory.search(query, user_id=user_id, limit=limit)\n",
    "            print(f\"🔍 Found {len(results)} relevant memories\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error searching memories: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_all_memories(self, user_id: str = \"default\") -> List[Dict]:\n",
    "        \"\"\"Get all memories for a user\"\"\"\n",
    "        try:\n",
    "            memories = self.memory.get_all(user_id=user_id)\n",
    "            print(f\"📚 Retrieved {len(memories)} total memories\")\n",
    "            return memories\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error retrieving memories: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def chat_with_memory(self, user_input: str, user_id: str = \"default\") -> Dict:\n",
    "        \"\"\"\n",
    "        Main function: Chat with AI that remembers previous conversations\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\n💬 User: {user_input}\")\n",
    "            \n",
    "            # Step 1: Search for relevant memories\n",
    "            relevant_memories = self.search_memories(user_input, user_id, limit=3)\n",
    "            \n",
    "            # Step 2: Build context from memories\n",
    "            memory_context = \"\"\n",
    "            if relevant_memories:\n",
    "                memory_context = \"Previous conversations and memories:\\n\"\n",
    "                for i, memory in enumerate(relevant_memories, 1):\n",
    "                    memory_text = memory.get('memory', memory.get('text', ''))\n",
    "                    memory_context += f\"{i}. {memory_text}\\n\"\n",
    "                memory_context += \"\\n\"\n",
    "            \n",
    "            # Step 3: Create prompt for Gemini\n",
    "            system_prompt = f\"\"\"You are a helpful AI assistant with memory of previous conversations.\n",
    "\n",
    "{memory_context}Current user message: {user_input}\n",
    "\n",
    "Instructions:\n",
    "- Use the previous memories to provide personalized responses\n",
    "- If memories are relevant, reference them naturally\n",
    "- Be conversational and helpful\n",
    "- If no relevant memories exist, respond normally but show you're learning about the user\"\"\"\n",
    "\n",
    "            # Step 4: Get response from Gemini\n",
    "            response = self.gemini_model.generate_content(system_prompt)\n",
    "            ai_response = response.text\n",
    "            \n",
    "            print(f\"🤖 AI: {ai_response}\")\n",
    "            \n",
    "            # Step 5: Store this conversation as a new memory\n",
    "            conversation_memory = f\"User said: '{user_input}' | AI replied: '{ai_response[:100]}...'\"\n",
    "            self.add_memory(conversation_memory, user_id)\n",
    "            \n",
    "            return {\n",
    "                \"user_input\": user_input,\n",
    "                \"ai_response\": ai_response,\n",
    "                \"memories_used\": len(relevant_memories),\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Sorry, I encountered an error: {str(e)}\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            return {\n",
    "                \"user_input\": user_input,\n",
    "                \"ai_response\": error_msg,\n",
    "                \"memories_used\": 0,\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "    \n",
    "    def add_fact(self, fact: str, user_id: str = \"default\") -> Dict:\n",
    "        \"\"\"Add a specific fact/preference to memory\"\"\"\n",
    "        fact_memory = f\"FACT: {fact}\"\n",
    "        return self.add_memory(fact_memory, user_id)\n",
    "    \n",
    "    def memory_summary(self, user_id: str = \"default\") -> str:\n",
    "        \"\"\"Get AI summary of all memories\"\"\"\n",
    "        try:\n",
    "            memories = self.get_all_memories(user_id)\n",
    "            \n",
    "            if not memories:\n",
    "                return \"No memories found.\"\n",
    "            \n",
    "            memory_texts = [mem.get('memory', mem.get('text', '')) for mem in memories]\n",
    "            combined_memories = \"\\n\".join(memory_texts)\n",
    "            \n",
    "            summary_prompt = f\"\"\"Analyze these memories and create a brief personality/preference summary:\n",
    "\n",
    "{combined_memories}\n",
    "\n",
    "Create a concise summary of what you know about this person including their preferences, habits, and key information.\"\"\"\n",
    "\n",
    "            response = self.gemini_model.generate_content(summary_prompt)\n",
    "            return response.text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error creating summary: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Demo of the Simple Memory AI\"\"\"\n",
    "    \n",
    "    # Setup - Replace with your actual Google API key\n",
    "    google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "    \n",
    "    if google_api_key == \"your-google-api-key-here\":\n",
    "        print(\"❌ Please replace 'your-google-api-key-here' with your actual Google API key\")\n",
    "        print(\"💡 Get free API key from: https://aistudio.google.com/app/apikey\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Initialize the AI\n",
    "        ai = SimpleMemoryAI(google_api_key=google_api_key)\n",
    "        \n",
    "        print(\"\\n🚀 Simple MemoryAI Demo Started!\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Add some initial facts about the user\n",
    "        print(\"\\n📝 Adding initial facts...\")\n",
    "        facts = [\n",
    "            \"I love pizza and Italian food\",\n",
    "            \"I work as a software developer\",\n",
    "            \"My favorite programming language is Python\",\n",
    "            \"I have a dog named Buddy\",\n",
    "            \"I like to exercise in the morning\"\n",
    "        ]\n",
    "        \n",
    "        for fact in facts:\n",
    "            ai.add_fact(fact, \"demo_user\")\n",
    "        \n",
    "        # Interactive chat demo\n",
    "        print(\"\\n💬 Starting conversation...\")\n",
    "        \n",
    "        # Simulate a conversation\n",
    "        conversations = [\n",
    "            \"What kind of food do I like?\",\n",
    "            \"Tell me about my pet\",\n",
    "            \"What do I do for work?\",\n",
    "            \"What programming language should I use for my new project?\",\n",
    "            \"When is the best time for me to exercise?\"\n",
    "        ]\n",
    "        \n",
    "        for question in conversations:\n",
    "            result = ai.chat_with_memory(question, \"demo_user\")\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        # Show memory summary\n",
    "        print(f\"\\n📊 Memory Summary:\")\n",
    "        summary = ai.memory_summary(\"demo_user\")\n",
    "        print(summary)\n",
    "        \n",
    "        print(\"\\n✅ Demo completed!\")\n",
    "        print(\"\\n💡 To use this system:\")\n",
    "        print(\"1. Replace the API key with your actual Google API key\")\n",
    "        print(\"2. Run: python main.py\")\n",
    "        print(\"3. The AI will remember everything across conversations!\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"❌ {e}\")\n",
    "        print(\"💡 Get your free Google API key from: https://aistudio.google.com/app/apikey\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {str(e)}\")\n",
    "\n",
    "# Interactive mode\n",
    "def interactive_mode():\n",
    "    \"\"\"Run in interactive chat mode\"\"\"\n",
    "    google_api_key = input(\"Enter your Google API key: \").strip()\n",
    "    user_id = input(\"Enter your user ID (or press Enter for 'default'): \").strip() or \"default\"\n",
    "    \n",
    "    try:\n",
    "        ai = SimpleMemoryAI(google_api_key=google_api_key)\n",
    "        \n",
    "        print(f\"\\n🤖 Hi! I'm your AI assistant with memory. I'll remember our conversations.\")\n",
    "        print(\"Type 'quit' to exit, 'summary' to see what I know about you\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(f\"\\n{user_id}: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"👋 Goodbye! I'll remember our conversation for next time.\")\n",
    "                break\n",
    "            elif user_input.lower() == 'summary':\n",
    "                print(\"\\n📊 Here's what I know about you:\")\n",
    "                summary = ai.memory_summary(user_id)\n",
    "                print(summary)\n",
    "            elif user_input:\n",
    "                ai.chat_with_memory(user_input, user_id)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Choose mode:\")\n",
    "    print(\"1. Demo mode (press Enter)\")\n",
    "    print(\"2. Interactive chat mode (type 'chat')\")\n",
    "    \n",
    "    choice = input(\"Your choice: \").strip().lower()\n",
    "    \n",
    "    if choice == 'chat':\n",
    "        interactive_mode()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mem0 import Memory\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"config\": {\n",
    "            \"model\": \"gemini-1.5-flash-latest\",\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_tokens\": 2000,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "m = Memory.from_config(config)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"How about a thriller movies? They can be quite engaging.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I’m not a big fan of thriller movies but I love sci-fi movies.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n",
    "]\n",
    "m.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
